import getpass
import os
import subprocess
import sys
import time

import requests

os.environ["HTTP_PROXY"] = ''
os.environ["HTTPS_PROXY"] = ''
os.environ["all_proxy"] = ''
os.environ["ALL_PROXY"] = ''

from llamafactory.extras.packages import is_gradio_available
if is_gradio_available():
    import gradio as gr


def get_ollama_model_list():
    '''
    获取ollama模型列表
    '''
    uer_name = getpass.getuser()
    if uer_name == 'docker':
        cmd_check = "docker exec -w /root/models/v3 ollama ollama list"
    else:
        cmd_check = "ollama list"
    pipe_check = subprocess.Popen(cmd_check, shell=True, stdout=subprocess.PIPE)
    pipe_check.wait()
    stdout_msg = pipe_check.stdout.read().decode()
    ollama_model_list = []
    for stdline in stdout_msg.split('\n')[1:-1]:
        ollama_model_list.append(stdline.split('\t')[0].strip())
    print(ollama_model_list)
    return ollama_model_list


def get_ollama_model_list_by_api():
    '''
    api 获取ollama模型列表
    '''
    url = 'http://127.0.0.1:11434/api/tags'
    res = requests.get(url).json()
    models: list = res.get('models', [])
    ollama_model_list = [model_info['name'] for model_info in models]
    print(ollama_model_list)
    return ollama_model_list


def convert_gguf(export_dir):
    '''
    convert to gguf

    gguf file save in export_dir
    '''
    current_python: str = sys.executable
    if getpass.getuser() == 'zj':
        convert_cmd = '{} ./convert_hf_to_gguf/convert_hf_to_gguf.py {}'.format(current_python, export_dir)
    else:
        convert_cmd = '/home/ubuntu/miniconda3/envs/llama_cpp/bin/python ./convert_hf_to_gguf/convert_hf_to_gguf.py {}'.format(export_dir)
    print('convert_cmd is {}'.format(convert_cmd))
    with subprocess.Popen(
            convert_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            shell=True
    ) as proc:
        print('convert to gguf start')
        # 实时输出日志
        result_msg = ''
        for stderr_info in iter(proc.stderr.readline, b''):
            stderr_info_str = stderr_info.decode()
            print(stderr_info_str)
        # proc.wait()
        # result_msg = proc.stderr.read().decode()
        # print('convert to gguf finished')
        # print('proc execute stderr is {}'.format(result_msg))
        gr.Info('模型转化完成!!')


def create_ollama_model(export_dir: str):
    '''

    '''
    new_model_name = os.path.basename(export_dir)

    ollama_models = get_ollama_model_list_by_api()
    if '{}:{}'.format(new_model_name, 'latest') in ollama_models:
        return f'模型已发布过了 {new_model_name}'
    gr.Info('开始发布模型： {}'.format(new_model_name))
    convert_gguf(export_dir)
    modelfile_content = '''# Modelfile generated by "ollama show"
FROM ./ggml-model-f16.gguf
TEMPLATE "{{ if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>

{{ .Response }}<|eot_id|>"
PARAMETER num_keep 24
PARAMETER stop <|start_header_id|>
PARAMETER stop <|end_header_id|>
PARAMETER stop <|eot_id|>
'''
    modelfile_path = os.path.join(export_dir, 'Modelfile')
    with open(modelfile_path, 'wt') as f:
        f.write(modelfile_content)
    uer_name = getpass.getuser()
    if uer_name == 'docker':
        create_ollama_cmd = 'docker exec -w /root/models/{} ollama ollama create {} -f Modelfile' \
            .format(new_model_name, new_model_name)
    else:
        create_ollama_cmd = 'ollama create {} -f Modelfile'.format(new_model_name)
    cwd = os.getcwd()
    new_cwd = os.path.join(cwd, export_dir)
    with subprocess.Popen(
            create_ollama_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            cwd=new_cwd,
            shell=True
    ) as proc2:
        print('create ollama model start')
        gr.Info('模型发布时间较长')
        create_ollama_cmd_msg = '模型发布失败: {}'.format('{}:{}'.format(new_model_name, 'latest'))
        for i in range(120):
            # check ollama model list every 5 seconds
            ollama_models = get_ollama_model_list_by_api()
            print('new_model_name: {}, ollama_models: {}'.format(new_model_name, ollama_models))
            if '{}:{}'.format(new_model_name, 'latest') in ollama_models:
                create_ollama_cmd_msg = '模型发布成功: {}'.format('{}:{}'.format(new_model_name, 'latest'))
                break
            else:
                time.sleep(5)
        print(create_ollama_cmd_msg)
        gr.Info(create_ollama_cmd_msg)
    return create_ollama_cmd_msg


if __name__ == '__main__':
    print('cwd is {}'.format(os.getcwd()))
    get_ollama_model_list_by_api()
    # convert_gguf('./models/v3')
    # create_ollama_model('./models/v3')
